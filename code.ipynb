{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMGqB0QuqoQPukTlYIaOihC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"76a55fb8372d46e09990df0fb096f395":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_72a8a3b366f24c6292281d1388ab7917","IPY_MODEL_02dc7572c9e3400b90447aa3c70ed432","IPY_MODEL_09a6d35a5d724e418c52aaa97b3c3228"],"layout":"IPY_MODEL_54de2c977e744868be7cef9dcebef8f0"}},"72a8a3b366f24c6292281d1388ab7917":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_18ab642d6ec84e1eb3aafca44169efc2","placeholder":"â€‹","style":"IPY_MODEL_1ba806d7c07d4de9b218acc5ed586544","value":"100%"}},"02dc7572c9e3400b90447aa3c70ed432":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9dad5d6e548d440baf27640c04dbb419","max":443,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f271e515826f4cb3a475e5380bca1355","value":443}},"09a6d35a5d724e418c52aaa97b3c3228":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e7d72fed76d4d01b13772f73efea8c8","placeholder":"â€‹","style":"IPY_MODEL_496a660d742149018d76d5f2e8a4f5cc","value":"â€‡443/443â€‡[03:10&lt;00:00,â€‡â€‡3.19it/s]"}},"54de2c977e744868be7cef9dcebef8f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"18ab642d6ec84e1eb3aafca44169efc2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ba806d7c07d4de9b218acc5ed586544":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9dad5d6e548d440baf27640c04dbb419":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f271e515826f4cb3a475e5380bca1355":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5e7d72fed76d4d01b13772f73efea8c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"496a660d742149018d76d5f2e8a4f5cc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YQbyLlhH1LMV","executionInfo":{"status":"ok","timestamp":1754744554033,"user_tz":-330,"elapsed":28,"user":{"displayName":"Arkadeb MANNA","userId":"10938977057359683247"}},"outputId":"209cd61c-0c38-4e60-af2b-a4b653dbe4f1"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}],"source":["import os\n","HOME = os.getcwd()\n","print(HOME)"]},{"cell_type":"code","source":["SOURCE_VIDEO_PATH = f\"/content/14101338_3840_2160_60fps.mp4\""],"metadata":{"id":"9pQKB69e1NjD","executionInfo":{"status":"ok","timestamp":1754744554036,"user_tz":-330,"elapsed":4,"user":{"displayName":"Arkadeb MANNA","userId":"10938977057359683247"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["!pip install \"ultralytics<=8.3.40\"\n","\n","from IPython import display\n","display.clear_output()\n","!yolo settings sync=False\n","\n","import ultralytics\n","ultralytics.checks()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kZERAXOi1PNz","executionInfo":{"status":"ok","timestamp":1754744570715,"user_tz":-330,"elapsed":16679,"user":{"displayName":"Arkadeb MANNA","userId":"10938977057359683247"}},"outputId":"15863887-1122-4de0-c3ca-f30e6981f9e5"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Ultralytics 8.3.40 ðŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n","Setup complete âœ… (2 CPUs, 12.7 GB RAM, 43.8/112.6 GB disk)\n"]}]},{"cell_type":"code","source":["%cd {HOME}\n","!git clone https://github.com/ifzhang/ByteTrack.git\n","%cd {HOME}/ByteTrack\n","\n","!sed -i 's/onnx==1.8.1/onnx==1.9.0/g' requirements.txt\n","\n","!pip3 install -q -r requirements.txt\n","!python3 setup.py -q develop\n","!pip install -q cython_bbox\n","!pip install -q onemetric\n","!pip install -q loguru lap thop\n","\n","from IPython import display\n","display.clear_output()\n","\n","\n","import sys\n","sys.path.append(f\"{HOME}/ByteTrack\")\n","\n","\n","import yolox\n","print(\"yolox.__version__:\", yolox.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lt5jgKLL1RgH","executionInfo":{"status":"ok","timestamp":1754744609620,"user_tz":-330,"elapsed":38901,"user":{"displayName":"Arkadeb MANNA","userId":"10938977057359683247"}},"outputId":"7397ff55-4c68-4b2a-bce8-d845c956e139"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["yolox.__version__: 0.1.0\n"]}]},{"cell_type":"code","source":["from yolox.tracker.byte_tracker import BYTETracker, STrack\n","from onemetric.cv.utils.iou import box_iou_batch\n","from dataclasses import dataclass\n","\n","\n","@dataclass(frozen=True)\n","class BYTETrackerArgs:\n","    track_thresh: float = 0.25\n","    track_buffer: int = 30\n","    match_thresh: float = 0.8\n","    aspect_ratio_thresh: float = 3.0\n","    min_box_area: float = 1.0\n","    mot20: bool = False"],"metadata":{"id":"rkVg-PYL1Thy","executionInfo":{"status":"ok","timestamp":1754744609720,"user_tz":-330,"elapsed":118,"user":{"displayName":"Arkadeb MANNA","userId":"10938977057359683247"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["!pip install torchreid"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GAD-_BuZ1WPf","executionInfo":{"status":"ok","timestamp":1754744615971,"user_tz":-330,"elapsed":6247,"user":{"displayName":"Arkadeb MANNA","userId":"10938977057359683247"}},"outputId":"77746c5e-a9e8-44be-dcd7-f75f08b07126"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchreid in /usr/local/lib/python3.11/dist-packages (0.2.5)\n"]}]},{"cell_type":"code","source":["import torch\n","import torchreid\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model_reid = torchreid.models.build_model(\n","    name='osnet_x1_0',\n","    num_classes=1000,\n","    pretrained=True\n",")\n","model_reid.eval().to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zBp9tgwk1YHs","executionInfo":{"status":"ok","timestamp":1754744621926,"user_tz":-330,"elapsed":5938,"user":{"displayName":"Arkadeb MANNA","userId":"10938977057359683247"}},"outputId":"6aaca556-c80e-453e-f62b-6fcd4a488fba"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchreid/reid/metrics/rank.py:11: UserWarning: Cython evaluation (very fast so highly recommended) is unavailable, now use python evaluation.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Successfully loaded imagenet pretrained weights from \"/root/.cache/torch/checkpoints/osnet_x1_0_imagenet.pth\"\n"]},{"output_type":"execute_result","data":{"text/plain":["OSNet(\n","  (conv1): ConvLayer(\n","    (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (relu): ReLU(inplace=True)\n","  )\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (conv2): Sequential(\n","    (0): OSBlock(\n","      (conv1): Conv1x1(\n","        (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (conv2a): LightConv3x3(\n","        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n","        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (conv2b): Sequential(\n","        (0): LightConv3x3(\n","          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n","          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (1): LightConv3x3(\n","          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n","          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (conv2c): Sequential(\n","        (0): LightConv3x3(\n","          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n","          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (1): LightConv3x3(\n","          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n","          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): LightConv3x3(\n","          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n","          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (conv2d): Sequential(\n","        (0): LightConv3x3(\n","          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n","          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (1): LightConv3x3(\n","          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n","          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): LightConv3x3(\n","          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n","          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): LightConv3x3(\n","          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n","          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (gate): ChannelGate(\n","        (global_avgpool): AdaptiveAvgPool2d(output_size=1)\n","        (fc1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\n","        (relu): ReLU(inplace=True)\n","        (fc2): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1))\n","        (gate_activation): Sigmoid()\n","      )\n","      (conv3): Conv1x1Linear(\n","        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (downsample): Conv1x1Linear(\n","        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): OSBlock(\n","      (conv1): Conv1x1(\n","        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (conv2a): LightConv3x3(\n","        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n","        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (conv2b): Sequential(\n","        (0): LightConv3x3(\n","          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n","          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (1): LightConv3x3(\n","          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n","          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (conv2c): Sequential(\n","        (0): LightConv3x3(\n","          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n","          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (1): LightConv3x3(\n","          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n","          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): LightConv3x3(\n","          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n","          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (conv2d): Sequential(\n","        (0): LightConv3x3(\n","          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n","          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (1): LightConv3x3(\n","          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n","          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): LightConv3x3(\n","          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n","          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): LightConv3x3(\n","          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n","          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (gate): ChannelGate(\n","        (global_avgpool): AdaptiveAvgPool2d(output_size=1)\n","        (fc1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\n","        (relu): ReLU(inplace=True)\n","        (fc2): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1))\n","        (gate_activation): Sigmoid()\n","      )\n","      (conv3): Conv1x1Linear(\n","        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (2): Sequential(\n","      (0): Conv1x1(\n","        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","    )\n","  )\n","  (conv3): Sequential(\n","    (0): OSBlock(\n","      (conv1): Conv1x1(\n","        (conv): Conv2d(256, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (conv2a): LightConv3x3(\n","        (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n","        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (conv2b): Sequential(\n","        (0): LightConv3x3(\n","          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n","          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (1): LightConv3x3(\n","          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n","          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (conv2c): Sequential(\n","        (0): LightConv3x3(\n","          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n","          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (1): LightConv3x3(\n","          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n","          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): LightConv3x3(\n","          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n","          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (conv2d): Sequential(\n","        (0): LightConv3x3(\n","          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n","          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (1): LightConv3x3(\n","          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n","          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): LightConv3x3(\n","          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n","          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): LightConv3x3(\n","          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n","          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (gate): ChannelGate(\n","        (global_avgpool): AdaptiveAvgPool2d(output_size=1)\n","        (fc1): Conv2d(96, 6, kernel_size=(1, 1), stride=(1, 1))\n","        (relu): ReLU(inplace=True)\n","        (fc2): Conv2d(6, 96, kernel_size=(1, 1), stride=(1, 1))\n","        (gate_activation): Sigmoid()\n","      )\n","      (conv3): Conv1x1Linear(\n","        (conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (downsample): Conv1x1Linear(\n","        (conv): Conv2d(256, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): OSBlock(\n","      (conv1): Conv1x1(\n","        (conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (conv2a): LightConv3x3(\n","        (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n","        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (conv2b): Sequential(\n","        (0): LightConv3x3(\n","          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n","          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (1): LightConv3x3(\n","          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n","          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (conv2c): Sequential(\n","        (0): LightConv3x3(\n","          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n","          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (1): LightConv3x3(\n","          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n","          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): LightConv3x3(\n","          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n","          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (conv2d): Sequential(\n","        (0): LightConv3x3(\n","          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n","          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (1): LightConv3x3(\n","          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n","          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): LightConv3x3(\n","          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n","          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): LightConv3x3(\n","          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n","          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (gate): ChannelGate(\n","        (global_avgpool): AdaptiveAvgPool2d(output_size=1)\n","        (fc1): Conv2d(96, 6, kernel_size=(1, 1), stride=(1, 1))\n","        (relu): ReLU(inplace=True)\n","        (fc2): Conv2d(6, 96, kernel_size=(1, 1), stride=(1, 1))\n","        (gate_activation): Sigmoid()\n","      )\n","      (conv3): Conv1x1Linear(\n","        (conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (2): Sequential(\n","      (0): Conv1x1(\n","        (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","    )\n","  )\n","  (conv4): Sequential(\n","    (0): OSBlock(\n","      (conv1): Conv1x1(\n","        (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (conv2a): LightConv3x3(\n","        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n","        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (conv2b): Sequential(\n","        (0): LightConv3x3(\n","          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n","          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (1): LightConv3x3(\n","          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n","          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (conv2c): Sequential(\n","        (0): LightConv3x3(\n","          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n","          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (1): LightConv3x3(\n","          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n","          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): LightConv3x3(\n","          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n","          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (conv2d): Sequential(\n","        (0): LightConv3x3(\n","          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n","          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (1): LightConv3x3(\n","          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n","          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): LightConv3x3(\n","          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n","          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): LightConv3x3(\n","          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n","          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (gate): ChannelGate(\n","        (global_avgpool): AdaptiveAvgPool2d(output_size=1)\n","        (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1))\n","        (relu): ReLU(inplace=True)\n","        (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1))\n","        (gate_activation): Sigmoid()\n","      )\n","      (conv3): Conv1x1Linear(\n","        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (downsample): Conv1x1Linear(\n","        (conv): Conv2d(384, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): OSBlock(\n","      (conv1): Conv1x1(\n","        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (conv2a): LightConv3x3(\n","        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n","        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (conv2b): Sequential(\n","        (0): LightConv3x3(\n","          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n","          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (1): LightConv3x3(\n","          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n","          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (conv2c): Sequential(\n","        (0): LightConv3x3(\n","          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n","          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (1): LightConv3x3(\n","          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n","          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): LightConv3x3(\n","          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n","          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (conv2d): Sequential(\n","        (0): LightConv3x3(\n","          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n","          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (1): LightConv3x3(\n","          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n","          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): LightConv3x3(\n","          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n","          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): LightConv3x3(\n","          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n","          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (gate): ChannelGate(\n","        (global_avgpool): AdaptiveAvgPool2d(output_size=1)\n","        (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1))\n","        (relu): ReLU(inplace=True)\n","        (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1))\n","        (gate_activation): Sigmoid()\n","      )\n","      (conv3): Conv1x1Linear(\n","        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","  )\n","  (conv5): Conv1x1(\n","    (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (relu): ReLU(inplace=True)\n","  )\n","  (global_avgpool): AdaptiveAvgPool2d(output_size=1)\n","  (fc): Sequential(\n","    (0): Linear(in_features=512, out_features=512, bias=True)\n","    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace=True)\n","  )\n","  (classifier): Linear(in_features=512, out_features=1000, bias=True)\n",")"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["!pip install supervision==0.1.0\n","\n","\n","from IPython import display\n","display.clear_output()\n","\n","\n","import supervision\n","print(\"supervision.__version__:\", supervision.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_s2J7MDX1bnA","executionInfo":{"status":"ok","timestamp":1754744626102,"user_tz":-330,"elapsed":4171,"user":{"displayName":"Arkadeb MANNA","userId":"10938977057359683247"}},"outputId":"92b78eed-fcd9-4ff8-9d67-31e5f19e3de7"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["supervision.__version__: 0.1.0\n"]}]},{"cell_type":"code","source":["from supervision.draw.color import ColorPalette\n","from supervision.geometry.dataclasses import Point\n","from supervision.video.dataclasses import VideoInfo\n","from supervision.video.source import get_video_frames_generator\n","from supervision.video.sink import VideoSink\n","from supervision.notebook.utils import show_frame_in_notebook\n","from supervision.tools.detections import Detections, BoxAnnotator\n","from supervision.tools.line_counter import LineCounter, LineCounterAnnotator"],"metadata":{"id":"1FPiQjUY1dlK","executionInfo":{"status":"ok","timestamp":1754744626123,"user_tz":-330,"elapsed":7,"user":{"displayName":"Arkadeb MANNA","userId":"10938977057359683247"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["from typing import List\n","import numpy as np\n","def detections2boxes(detections: Detections) -> np.ndarray:\n","    return np.hstack((\n","        detections.xyxy,\n","        detections.confidence[:, np.newaxis]\n","    ))\n","def tracks2boxes(tracks: List[STrack]) -> np.ndarray:\n","    return np.array([\n","        track.tlbr\n","        for track\n","        in tracks\n","    ], dtype=float)\n","def match_detections_with_tracks(\n","    detections: Detections,\n","    tracks: List[STrack]\n",") -> Detections:\n","    if not np.any(detections.xyxy) or len(tracks) == 0:\n","        return np.empty((0,))\n","\n","    tracks_boxes = tracks2boxes(tracks=tracks)\n","    iou = box_iou_batch(tracks_boxes, detections.xyxy)\n","    track2detection = np.argmax(iou, axis=1)\n","\n","    tracker_ids = [None] * len(detections)\n","\n","    for tracker_index, detection_index in enumerate(track2detection):\n","        if iou[tracker_index, detection_index] != 0:\n","            original_id = tracks[tracker_index].track_id\n","            wrapped_id = (original_id - 1) % 50 + 1\n","            tracker_ids[detection_index] = wrapped_id\n","\n","    return tracker_ids"],"metadata":{"id":"FNjRMCc51frh","executionInfo":{"status":"ok","timestamp":1754744626138,"user_tz":-330,"elapsed":6,"user":{"displayName":"Arkadeb MANNA","userId":"10938977057359683247"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["MODEL = \"yolov8x.pt\"\n","from ultralytics import YOLO\n","\n","model = YOLO(MODEL)\n","model.fuse()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RBjrLk6G1h12","executionInfo":{"status":"ok","timestamp":1754744627663,"user_tz":-330,"elapsed":1516,"user":{"displayName":"Arkadeb MANNA","userId":"10938977057359683247"}},"outputId":"b021a4a6-da1b-49ce-cc23-f9eee08d9fe3"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["YOLOv8x summary (fused): 268 layers, 68,200,608 parameters, 0 gradients, 257.8 GFLOPs\n"]}]},{"cell_type":"code","source":["from PIL import Image\n","import torch\n","from torchvision import transforms\n","\n","reid_transform = transforms.Compose([\n","    transforms.Resize((256, 128)),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406],\n","                         [0.229, 0.224, 0.225]),\n","])\n","\n","def extract_reid_features(frame, bboxes, model):\n","    crops = []\n","    for box in bboxes:\n","        x1, y1, x2, y2 = map(int, box)\n","        crop = frame[y1:y2, x1:x2]\n","        if crop.size == 0:\n","            continue\n","        crop = Image.fromarray(crop[..., ::-1])\n","        crop = reid_transform(crop).unsqueeze(0).cuda()\n","        crops.append(crop)\n","\n","    if not crops:\n","        return np.array([])\n","\n","    with torch.no_grad():\n","        inputs = torch.cat(crops, dim=0)\n","        features = model(inputs).cpu().numpy()\n","\n","    return features"],"metadata":{"id":"7vQTYKfA1j0s","executionInfo":{"status":"ok","timestamp":1754744627666,"user_tz":-330,"elapsed":38,"user":{"displayName":"Arkadeb MANNA","userId":"10938977057359683247"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["LINE_START = Point(50, 1500)\n","LINE_END = Point(3840-50, 1500)\n","\n","TARGET_VIDEO_PATH = f\"{HOME}/vehicle-counting-result.mp4\""],"metadata":{"id":"li4LTtiW1mF9","executionInfo":{"status":"ok","timestamp":1754744627670,"user_tz":-330,"elapsed":27,"user":{"displayName":"Arkadeb MANNA","userId":"10938977057359683247"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["VideoInfo.from_video_path(SOURCE_VIDEO_PATH)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5QMJAibP1oDU","executionInfo":{"status":"ok","timestamp":1754744627691,"user_tz":-330,"elapsed":39,"user":{"displayName":"Arkadeb MANNA","userId":"10938977057359683247"}},"outputId":"7393ca94-3180-4914-8591-2676b2fda93e"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["VideoInfo(width=3840, height=2160, fps=60, total_frames=443)"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["import numpy as np\n","np.float = float"],"metadata":{"id":"QEKtYKPt1p-R","executionInfo":{"status":"ok","timestamp":1754744627720,"user_tz":-330,"elapsed":19,"user":{"displayName":"Arkadeb MANNA","userId":"10938977057359683247"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["CLASS_NAMES_DICT = model.model.names\n","CLASS_ID = [0]"],"metadata":{"id":"zowaDGBQ1sBg","executionInfo":{"status":"ok","timestamp":1754744627729,"user_tz":-330,"elapsed":5,"user":{"displayName":"Arkadeb MANNA","userId":"10938977057359683247"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["from tqdm.notebook import tqdm\n","\n","byte_tracker = BYTETracker(BYTETrackerArgs())\n","video_info = VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n","generator = get_video_frames_generator(SOURCE_VIDEO_PATH)\n","line_counter = LineCounter(start=LINE_START, end=LINE_END)\n","box_annotator = BoxAnnotator(color=ColorPalette(), thickness=4, text_thickness=4, text_scale=2)\n","line_annotator = LineCounterAnnotator(thickness=4, text_thickness=4, text_scale=2)\n","\n","import csv\n","from collections import defaultdict\n","\n","entry_times = dict()\n","last_seen_frame = dict()\n","fps = video_info.fps\n","frame_idx = 0\n","final_log = []\n","\n","with VideoSink(TARGET_VIDEO_PATH, video_info) as sink:\n","    for frame in tqdm(generator, total=video_info.total_frames):\n","        results = model(frame)\n","        detections = Detections(\n","            xyxy=results[0].boxes.xyxy.cpu().numpy(),\n","            confidence=results[0].boxes.conf.cpu().numpy(),\n","            class_id=results[0].boxes.cls.cpu().numpy().astype(int)\n","        )\n","        mask = np.array([class_id in CLASS_ID for class_id in detections.class_id], dtype=bool)\n","        detections.filter(mask=mask, inplace=True)\n","        bboxes = detections.xyxy\n","        features = extract_reid_features(frame, bboxes, model_reid)\n","\n","        if len(features) > 0:\n","            detection_features = np.hstack((detections2boxes(detections), features))\n","        else:\n","            detection_features = detections2boxes(detections)\n","\n","        tracks = byte_tracker.update(\n","        output_results=detection_features,\n","        img_info=frame.shape,\n","        img_size=frame.shape,\n","        frame=frame,\n","        reid_model=model_reid\n","        )\n","        current_ids = set()\n","\n","        for track in tracks:\n","            if not track.is_activated:\n","                continue\n","            original_id = track.track_id\n","            track_id = (original_id - 1) % 50 + 1\n","            current_ids.add(track_id)\n","\n","            if track_id not in entry_times:\n","                entry_times[track_id] = frame_idx\n","\n","            last_seen_frame[track_id] = frame_idx\n","\n","        inactive_ids = []\n","        for track_id, last_frame in last_seen_frame.items():\n","            if track_id not in current_ids and frame_idx - last_frame > byte_tracker.args.track_buffer:\n","                entry_sec = entry_times[track_id] / fps\n","                exit_sec = last_frame / fps\n","                final_log.append((track_id, entry_sec, exit_sec))\n","                inactive_ids.append(track_id)\n","\n","        for track_id in inactive_ids:\n","            entry_times.pop(track_id, None)\n","            last_seen_frame.pop(track_id, None)\n","\n","        tracker_id = match_detections_with_tracks(detections=detections, tracks=tracks)\n","        detections.tracker_id = np.array(tracker_id)\n","        mask = np.array([tracker_id is not None for tracker_id in detections.tracker_id], dtype=bool)\n","        detections.filter(mask=mask, inplace=True)\n","        labels = [\n","            f\"#{tracker_id} {CLASS_NAMES_DICT[class_id]} {confidence:0.2f}\"\n","            for _, confidence, class_id, tracker_id\n","            in detections\n","        ]\n","        line_counter.update(detections=detections)\n","        frame = box_annotator.annotate(frame=frame, detections=detections, labels=labels)\n","        sink.write_frame(frame)\n","        frame_idx += 1\n","import pandas as pd\n","\n","for track_id in list(last_seen_frame):\n","    entry_sec = entry_times[track_id] / fps\n","    exit_sec = last_seen_frame[track_id] / fps\n","    final_log.append((track_id, entry_sec, exit_sec))\n","\n","df = pd.DataFrame(final_log, columns=[\"person_id\", \"entry_time\", \"exit_time\"])\n","df[\"entry_time\"] = pd.to_datetime(df[\"entry_time\"], unit='s').dt.strftime('%H:%M:%S')\n","df[\"exit_time\"] = pd.to_datetime(df[\"exit_time\"], unit='s').dt.strftime('%H:%M:%S')\n","\n","df.to_csv(\"person_entry_exit_log.csv\", index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["76a55fb8372d46e09990df0fb096f395","72a8a3b366f24c6292281d1388ab7917","02dc7572c9e3400b90447aa3c70ed432","09a6d35a5d724e418c52aaa97b3c3228","54de2c977e744868be7cef9dcebef8f0","18ab642d6ec84e1eb3aafca44169efc2","1ba806d7c07d4de9b218acc5ed586544","9dad5d6e548d440baf27640c04dbb419","f271e515826f4cb3a475e5380bca1355","5e7d72fed76d4d01b13772f73efea8c8","496a660d742149018d76d5f2e8a4f5cc"]},"id":"Z-ss-FHr1t0F","executionInfo":{"status":"ok","timestamp":1754744818145,"user_tz":-330,"elapsed":190393,"user":{"displayName":"Arkadeb MANNA","userId":"10938977057359683247"}},"outputId":"077775d6-7cc4-4bf6-dbdf-c8e38a5e96cd"},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/443 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76a55fb8372d46e09990df0fb096f395"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 17 persons, 2 bicycles, 3 cars, 1 motorcycle, 1 backpack, 66.2ms\n","Speed: 2.7ms preprocess, 66.2ms inference, 233.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 18 persons, 2 bicycles, 4 cars, 1 motorcycle, 1 backpack, 1 handbag, 60.5ms\n","Speed: 10.7ms preprocess, 60.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 18 persons, 2 bicycles, 5 cars, 1 motorcycle, 1 backpack, 1 handbag, 67.5ms\n","Speed: 4.9ms preprocess, 67.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 18 persons, 4 bicycles, 3 cars, 1 motorcycle, 1 backpack, 1 handbag, 65.4ms\n","Speed: 7.9ms preprocess, 65.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 17 persons, 4 bicycles, 4 cars, 1 motorcycle, 1 backpack, 1 handbag, 63.1ms\n","Speed: 3.6ms preprocess, 63.1ms inference, 5.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 18 persons, 3 bicycles, 3 cars, 1 motorcycle, 1 backpack, 2 handbags, 63.1ms\n","Speed: 3.5ms preprocess, 63.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 18 persons, 3 bicycles, 2 cars, 1 motorcycle, 1 backpack, 2 handbags, 89.8ms\n","Speed: 7.7ms preprocess, 89.8ms inference, 9.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 19 persons, 3 bicycles, 2 cars, 1 motorcycle, 1 backpack, 2 handbags, 1 cup, 89.1ms\n","Speed: 3.5ms preprocess, 89.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 18 persons, 4 bicycles, 3 cars, 1 motorcycle, 1 backpack, 2 handbags, 1 cup, 68.9ms\n","Speed: 14.7ms preprocess, 68.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 18 persons, 4 bicycles, 3 cars, 1 motorcycle, 2 backpacks, 1 handbag, 1 cup, 63.1ms\n","Speed: 14.0ms preprocess, 63.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 16 persons, 3 bicycles, 4 cars, 1 motorcycle, 2 backpacks, 1 handbag, 1 cup, 63.1ms\n","Speed: 6.8ms preprocess, 63.1ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 16 persons, 2 bicycles, 2 cars, 1 motorcycle, 2 backpacks, 2 handbags, 67.5ms\n","Speed: 5.6ms preprocess, 67.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 16 persons, 3 bicycles, 2 cars, 1 motorcycle, 2 backpacks, 1 handbag, 1 cup, 64.2ms\n","Speed: 8.5ms preprocess, 64.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 16 persons, 2 bicycles, 2 cars, 1 motorcycle, 2 backpacks, 2 handbags, 64.0ms\n","Speed: 6.7ms preprocess, 64.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 15 persons, 4 bicycles, 2 cars, 1 motorcycle, 2 backpacks, 1 handbag, 59.2ms\n","Speed: 3.8ms preprocess, 59.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 16 persons, 2 bicycles, 2 cars, 1 motorcycle, 1 backpack, 1 handbag, 60.8ms\n","Speed: 5.3ms preprocess, 60.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 17 persons, 2 bicycles, 3 cars, 1 motorcycle, 1 backpack, 1 handbag, 76.4ms\n","Speed: 3.5ms preprocess, 76.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 15 persons, 3 bicycles, 3 cars, 1 motorcycle, 1 backpack, 2 handbags, 63.1ms\n","Speed: 6.6ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 16 persons, 4 bicycles, 3 cars, 1 motorcycle, 1 backpack, 2 handbags, 65.6ms\n","Speed: 4.6ms preprocess, 65.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 17 persons, 3 bicycles, 4 cars, 1 motorcycle, 1 traffic light, 1 backpack, 2 handbags, 63.1ms\n","Speed: 3.5ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 16 persons, 5 bicycles, 3 cars, 1 motorcycle, 1 traffic light, 1 backpack, 2 handbags, 59.2ms\n","Speed: 3.5ms preprocess, 59.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 17 persons, 3 bicycles, 3 cars, 1 motorcycle, 1 traffic light, 1 backpack, 1 handbag, 50.5ms\n","Speed: 3.5ms preprocess, 50.5ms inference, 5.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 16 persons, 2 bicycles, 3 cars, 1 motorcycle, 1 backpack, 48.7ms\n","Speed: 4.2ms preprocess, 48.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 16 persons, 2 bicycles, 3 cars, 1 motorcycle, 1 backpack, 1 handbag, 46.5ms\n","Speed: 3.6ms preprocess, 46.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 15 persons, 3 bicycles, 3 cars, 1 motorcycle, 1 backpack, 1 handbag, 46.5ms\n","Speed: 3.6ms preprocess, 46.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 15 persons, 3 bicycles, 2 cars, 1 motorcycle, 1 backpack, 1 handbag, 46.5ms\n","Speed: 3.5ms preprocess, 46.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 16 persons, 1 bicycle, 3 cars, 1 motorcycle, 1 backpack, 1 handbag, 46.5ms\n","Speed: 3.6ms preprocess, 46.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 17 persons, 1 bicycle, 4 cars, 1 motorcycle, 1 backpack, 1 handbag, 46.5ms\n","Speed: 3.5ms preprocess, 46.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 15 persons, 1 bicycle, 3 cars, 1 motorcycle, 1 backpack, 1 handbag, 45.2ms\n","Speed: 5.0ms preprocess, 45.2ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 17 persons, 1 bicycle, 3 cars, 1 motorcycle, 1 backpack, 1 handbag, 45.2ms\n","Speed: 3.5ms preprocess, 45.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 15 persons, 1 bicycle, 2 cars, 1 motorcycle, 1 backpack, 2 handbags, 47.7ms\n","Speed: 3.4ms preprocess, 47.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 14 persons, 1 bicycle, 3 cars, 1 motorcycle, 1 backpack, 1 handbag, 1 vase, 53.7ms\n","Speed: 5.4ms preprocess, 53.7ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 15 persons, 1 bicycle, 1 car, 1 motorcycle, 1 backpack, 1 handbag, 63.1ms\n","Speed: 3.6ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 16 persons, 1 bicycle, 3 cars, 1 motorcycle, 1 backpack, 1 handbag, 63.1ms\n","Speed: 3.5ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 16 persons, 1 bicycle, 2 cars, 1 motorcycle, 1 backpack, 1 handbag, 63.1ms\n","Speed: 3.5ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 18 persons, 1 bicycle, 3 cars, 1 motorcycle, 1 backpack, 1 handbag, 63.1ms\n","Speed: 4.1ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 17 persons, 1 bicycle, 3 cars, 1 motorcycle, 1 backpack, 1 handbag, 63.1ms\n","Speed: 3.5ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 18 persons, 1 bicycle, 3 cars, 1 motorcycle, 1 backpack, 2 handbags, 63.1ms\n","Speed: 3.5ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 18 persons, 3 cars, 1 motorcycle, 1 backpack, 2 handbags, 63.1ms\n","Speed: 3.5ms preprocess, 63.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 17 persons, 3 cars, 1 motorcycle, 1 backpack, 2 handbags, 63.1ms\n","Speed: 3.5ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 18 persons, 3 cars, 1 motorcycle, 1 backpack, 1 handbag, 63.1ms\n","Speed: 4.0ms preprocess, 63.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 18 persons, 3 cars, 1 motorcycle, 1 backpack, 1 handbag, 63.1ms\n","Speed: 3.5ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 18 persons, 3 cars, 1 motorcycle, 1 backpack, 1 handbag, 63.0ms\n","Speed: 3.5ms preprocess, 63.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 17 persons, 3 cars, 1 motorcycle, 1 backpack, 1 handbag, 48.0ms\n","Speed: 4.1ms preprocess, 48.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 17 persons, 3 cars, 1 motorcycle, 1 backpack, 1 handbag, 47.9ms\n","Speed: 3.8ms preprocess, 47.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 17 persons, 2 cars, 1 motorcycle, 1 backpack, 1 handbag, 47.9ms\n","Speed: 3.9ms preprocess, 47.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 17 persons, 2 cars, 1 motorcycle, 1 backpack, 1 handbag, 47.2ms\n","Speed: 3.6ms preprocess, 47.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 16 persons, 1 car, 1 motorcycle, 1 backpack, 1 handbag, 49.9ms\n","Speed: 3.6ms preprocess, 49.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 16 persons, 1 car, 1 motorcycle, 1 backpack, 2 handbags, 45.2ms\n","Speed: 3.5ms preprocess, 45.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 14 persons, 1 car, 1 motorcycle, 1 backpack, 1 handbag, 44.0ms\n","Speed: 3.7ms preprocess, 44.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 15 persons, 1 car, 1 motorcycle, 1 backpack, 1 handbag, 44.0ms\n","Speed: 3.6ms preprocess, 44.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 14 persons, 1 car, 1 motorcycle, 1 backpack, 1 handbag, 43.9ms\n","Speed: 3.4ms preprocess, 43.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 15 persons, 2 cars, 1 motorcycle, 1 backpack, 44.0ms\n","Speed: 3.7ms preprocess, 44.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 16 persons, 4 cars, 1 motorcycle, 1 backpack, 1 handbag, 44.0ms\n","Speed: 3.5ms preprocess, 44.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 17 persons, 3 cars, 1 motorcycle, 1 backpack, 1 handbag, 45.3ms\n","Speed: 3.9ms preprocess, 45.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 18 persons, 5 cars, 1 motorcycle, 1 backpack, 1 handbag, 47.3ms\n","Speed: 3.5ms preprocess, 47.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 19 persons, 3 cars, 1 motorcycle, 1 backpack, 1 handbag, 49.4ms\n","Speed: 3.5ms preprocess, 49.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 18 persons, 4 cars, 1 motorcycle, 1 backpack, 2 handbags, 51.9ms\n","Speed: 3.4ms preprocess, 51.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 18 persons, 4 cars, 1 motorcycle, 1 backpack, 2 handbags, 53.7ms\n","Speed: 3.5ms preprocess, 53.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 17 persons, 4 cars, 1 motorcycle, 1 backpack, 2 handbags, 48.2ms\n","Speed: 4.8ms preprocess, 48.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 19 persons, 4 cars, 1 motorcycle, 1 backpack, 2 handbags, 48.4ms\n","Speed: 3.5ms preprocess, 48.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 16 persons, 3 cars, 1 motorcycle, 1 backpack, 1 handbag, 52.3ms\n","Speed: 3.9ms preprocess, 52.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 16 persons, 2 cars, 1 motorcycle, 1 backpack, 1 handbag, 66.0ms\n","Speed: 4.6ms preprocess, 66.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 15 persons, 2 cars, 1 motorcycle, 1 backpack, 1 handbag, 52.8ms\n","Speed: 3.3ms preprocess, 52.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 16 persons, 1 car, 1 motorcycle, 1 backpack, 1 handbag, 48.5ms\n","Speed: 3.6ms preprocess, 48.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 16 persons, 1 car, 1 motorcycle, 1 backpack, 2 handbags, 47.9ms\n","Speed: 7.8ms preprocess, 47.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 17 persons, 2 cars, 1 motorcycle, 1 backpack, 1 handbag, 47.9ms\n","Speed: 3.5ms preprocess, 47.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 16 persons, 2 cars, 1 motorcycle, 1 backpack, 1 handbag, 47.9ms\n","Speed: 3.6ms preprocess, 47.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 16 persons, 1 car, 2 motorcycles, 1 backpack, 1 handbag, 47.9ms\n","Speed: 3.9ms preprocess, 47.9ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 15 persons, 1 car, 1 motorcycle, 1 backpack, 1 handbag, 46.5ms\n","Speed: 3.4ms preprocess, 46.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 15 persons, 1 car, 1 motorcycle, 1 backpack, 1 handbag, 46.5ms\n","Speed: 3.4ms preprocess, 46.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 15 persons, 1 car, 2 motorcycles, 1 backpack, 1 handbag, 46.5ms\n","Speed: 10.6ms preprocess, 46.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 17 persons, 1 car, 2 motorcycles, 1 backpack, 1 handbag, 46.5ms\n","Speed: 3.6ms preprocess, 46.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 16 persons, 1 car, 2 motorcycles, 1 backpack, 1 handbag, 45.9ms\n","Speed: 3.6ms preprocess, 45.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 16 persons, 2 cars, 1 motorcycle, 1 backpack, 1 handbag, 45.2ms\n","Speed: 3.5ms preprocess, 45.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 14 persons, 1 bicycle, 2 cars, 1 motorcycle, 1 backpack, 1 handbag, 45.2ms\n","Speed: 4.2ms preprocess, 45.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 14 persons, 1 bicycle, 3 cars, 1 motorcycle, 1 backpack, 45.2ms\n","Speed: 3.5ms preprocess, 45.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 14 persons, 3 cars, 1 motorcycle, 1 backpack, 45.2ms\n","Speed: 3.4ms preprocess, 45.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 1 bicycle, 3 cars, 1 motorcycle, 1 backpack, 45.2ms\n","Speed: 3.6ms preprocess, 45.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 3 cars, 1 motorcycle, 1 backpack, 46.9ms\n","Speed: 3.5ms preprocess, 46.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 14 persons, 1 bicycle, 4 cars, 1 motorcycle, 1 backpack, 47.9ms\n","Speed: 3.7ms preprocess, 47.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 1 bicycle, 2 cars, 1 motorcycle, 1 backpack, 51.1ms\n","Speed: 3.8ms preprocess, 51.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 1 bicycle, 2 cars, 1 motorcycle, 1 backpack, 47.9ms\n","Speed: 3.6ms preprocess, 47.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 1 bicycle, 3 cars, 1 motorcycle, 1 backpack, 47.9ms\n","Speed: 3.5ms preprocess, 47.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 1 bicycle, 3 cars, 1 motorcycle, 1 backpack, 47.9ms\n","Speed: 3.8ms preprocess, 47.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 1 bicycle, 3 cars, 1 motorcycle, 1 backpack, 46.5ms\n","Speed: 3.7ms preprocess, 46.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 1 bicycle, 3 cars, 1 motorcycle, 1 backpack, 46.5ms\n","Speed: 3.7ms preprocess, 46.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 1 bicycle, 2 cars, 1 motorcycle, 1 backpack, 46.5ms\n","Speed: 3.5ms preprocess, 46.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 1 bicycle, 2 cars, 1 motorcycle, 1 backpack, 46.6ms\n","Speed: 3.6ms preprocess, 46.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 1 bicycle, 3 cars, 1 motorcycle, 1 backpack, 1 handbag, 46.5ms\n","Speed: 3.9ms preprocess, 46.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 1 bicycle, 3 cars, 1 motorcycle, 1 backpack, 1 handbag, 49.4ms\n","Speed: 3.6ms preprocess, 49.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 1 bicycle, 2 cars, 1 motorcycle, 1 backpack, 50.4ms\n","Speed: 3.8ms preprocess, 50.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 15 persons, 3 cars, 1 motorcycle, 1 backpack, 54.7ms\n","Speed: 4.1ms preprocess, 54.7ms inference, 8.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 15 persons, 1 bicycle, 3 cars, 1 motorcycle, 1 backpack, 57.9ms\n","Speed: 4.3ms preprocess, 57.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 15 persons, 1 bicycle, 3 cars, 1 motorcycle, 1 backpack, 63.1ms\n","Speed: 3.5ms preprocess, 63.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 16 persons, 1 bicycle, 3 cars, 1 motorcycle, 1 backpack, 64.7ms\n","Speed: 3.6ms preprocess, 64.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 16 persons, 3 cars, 1 motorcycle, 55.9ms\n","Speed: 4.8ms preprocess, 55.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 15 persons, 3 cars, 1 motorcycle, 53.8ms\n","Speed: 3.4ms preprocess, 53.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 15 persons, 1 bicycle, 3 cars, 1 motorcycle, 49.4ms\n","Speed: 3.7ms preprocess, 49.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 15 persons, 3 cars, 1 motorcycle, 49.5ms\n","Speed: 3.6ms preprocess, 49.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 15 persons, 3 cars, 1 motorcycle, 49.5ms\n","Speed: 3.8ms preprocess, 49.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 15 persons, 1 bicycle, 2 cars, 1 motorcycle, 49.5ms\n","Speed: 3.8ms preprocess, 49.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 3 cars, 1 motorcycle, 49.4ms\n","Speed: 3.5ms preprocess, 49.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 14 persons, 2 cars, 1 motorcycle, 49.4ms\n","Speed: 3.5ms preprocess, 49.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 15 persons, 3 cars, 1 motorcycle, 49.4ms\n","Speed: 3.6ms preprocess, 49.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 14 persons, 3 cars, 1 motorcycle, 50.5ms\n","Speed: 3.7ms preprocess, 50.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 15 persons, 2 cars, 1 motorcycle, 51.9ms\n","Speed: 7.5ms preprocess, 51.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 14 persons, 1 bicycle, 3 cars, 1 motorcycle, 54.8ms\n","Speed: 3.6ms preprocess, 54.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 15 persons, 3 cars, 1 motorcycle, 58.0ms\n","Speed: 3.6ms preprocess, 58.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 16 persons, 3 cars, 1 motorcycle, 1 handbag, 47.2ms\n","Speed: 5.7ms preprocess, 47.2ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 17 persons, 3 cars, 1 motorcycle, 47.2ms\n","Speed: 3.6ms preprocess, 47.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 18 persons, 3 cars, 1 motorcycle, 47.3ms\n","Speed: 3.6ms preprocess, 47.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 16 persons, 3 cars, 1 motorcycle, 1 handbag, 47.2ms\n","Speed: 3.5ms preprocess, 47.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 18 persons, 3 cars, 1 motorcycle, 47.2ms\n","Speed: 3.3ms preprocess, 47.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 15 persons, 3 cars, 1 motorcycle, 60.4ms\n","Speed: 3.6ms preprocess, 60.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 16 persons, 3 cars, 2 motorcycles, 1 handbag, 50.2ms\n","Speed: 3.5ms preprocess, 50.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 14 persons, 3 cars, 1 motorcycle, 1 handbag, 50.3ms\n","Speed: 3.7ms preprocess, 50.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 17 persons, 3 cars, 1 motorcycle, 2 handbags, 50.2ms\n","Speed: 3.5ms preprocess, 50.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 18 persons, 3 cars, 1 motorcycle, 2 handbags, 49.5ms\n","Speed: 3.4ms preprocess, 49.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 17 persons, 4 cars, 2 motorcycles, 2 handbags, 45.9ms\n","Speed: 3.6ms preprocess, 45.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 15 persons, 4 cars, 1 motorcycle, 1 handbag, 46.9ms\n","Speed: 3.4ms preprocess, 46.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 14 persons, 4 cars, 1 motorcycle, 2 handbags, 51.2ms\n","Speed: 3.8ms preprocess, 51.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 15 persons, 5 cars, 1 motorcycle, 1 handbag, 54.7ms\n","Speed: 7.2ms preprocess, 54.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 4 cars, 2 motorcycles, 1 handbag, 59.5ms\n","Speed: 3.6ms preprocess, 59.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 4 cars, 1 motorcycle, 1 handbag, 62.7ms\n","Speed: 3.2ms preprocess, 62.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 4 cars, 1 motorcycle, 1 handbag, 61.7ms\n","Speed: 3.7ms preprocess, 61.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 4 cars, 1 motorcycle, 1 handbag, 63.6ms\n","Speed: 3.5ms preprocess, 63.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 4 cars, 1 motorcycle, 1 handbag, 61.7ms\n","Speed: 3.7ms preprocess, 61.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 5 cars, 1 motorcycle, 1 handbag, 66.5ms\n","Speed: 6.4ms preprocess, 66.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 5 cars, 1 motorcycle, 1 handbag, 56.9ms\n","Speed: 3.5ms preprocess, 56.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 4 cars, 1 motorcycle, 1 handbag, 56.9ms\n","Speed: 3.9ms preprocess, 56.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 14 persons, 4 cars, 1 motorcycle, 1 handbag, 56.9ms\n","Speed: 4.2ms preprocess, 56.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 5 cars, 1 motorcycle, 1 handbag, 56.8ms\n","Speed: 3.6ms preprocess, 56.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 4 cars, 1 motorcycle, 2 handbags, 56.8ms\n","Speed: 3.4ms preprocess, 56.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 5 cars, 2 motorcycles, 1 handbag, 52.8ms\n","Speed: 3.7ms preprocess, 52.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 5 cars, 2 motorcycles, 1 handbag, 52.8ms\n","Speed: 3.5ms preprocess, 52.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 4 cars, 1 motorcycle, 1 handbag, 52.8ms\n","Speed: 3.6ms preprocess, 52.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 4 cars, 1 motorcycle, 1 handbag, 51.9ms\n","Speed: 3.5ms preprocess, 51.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 4 cars, 1 motorcycle, 1 handbag, 47.2ms\n","Speed: 3.5ms preprocess, 47.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 4 cars, 1 motorcycle, 3 handbags, 47.2ms\n","Speed: 3.5ms preprocess, 47.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 14 persons, 4 cars, 1 motorcycle, 3 handbags, 47.2ms\n","Speed: 3.6ms preprocess, 47.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 4 cars, 1 motorcycle, 2 handbags, 47.2ms\n","Speed: 3.6ms preprocess, 47.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 4 cars, 1 motorcycle, 1 handbag, 47.3ms\n","Speed: 3.5ms preprocess, 47.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 4 cars, 1 motorcycle, 2 handbags, 61.7ms\n","Speed: 4.1ms preprocess, 61.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 14 persons, 4 cars, 1 motorcycle, 2 handbags, 54.7ms\n","Speed: 3.5ms preprocess, 54.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 4 cars, 1 motorcycle, 1 traffic light, 1 handbag, 49.4ms\n","Speed: 3.7ms preprocess, 49.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 4 cars, 1 motorcycle, 1 traffic light, 1 handbag, 48.6ms\n","Speed: 3.6ms preprocess, 48.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 4 cars, 1 motorcycle, 1 handbag, 48.6ms\n","Speed: 3.5ms preprocess, 48.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 10 persons, 5 cars, 1 motorcycle, 1 handbag, 48.6ms\n","Speed: 3.6ms preprocess, 48.6ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 10 persons, 4 cars, 1 motorcycle, 1 handbag, 48.6ms\n","Speed: 3.6ms preprocess, 48.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 9 persons, 4 cars, 1 motorcycle, 1 handbag, 48.6ms\n","Speed: 3.5ms preprocess, 48.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 4 cars, 1 motorcycle, 2 handbags, 55.5ms\n","Speed: 3.5ms preprocess, 55.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 4 cars, 1 motorcycle, 2 handbags, 53.7ms\n","Speed: 3.7ms preprocess, 53.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 4 cars, 1 motorcycle, 2 handbags, 53.0ms\n","Speed: 3.7ms preprocess, 53.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 6 cars, 1 motorcycle, 2 handbags, 52.8ms\n","Speed: 3.6ms preprocess, 52.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 4 cars, 2 motorcycles, 2 handbags, 52.8ms\n","Speed: 3.6ms preprocess, 52.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 2 cars, 2 motorcycles, 2 handbags, 58.1ms\n","Speed: 3.5ms preprocess, 58.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 1 bicycle, 2 cars, 1 motorcycle, 2 handbags, 52.8ms\n","Speed: 4.6ms preprocess, 52.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 2 cars, 1 motorcycle, 2 handbags, 52.8ms\n","Speed: 6.4ms preprocess, 52.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 2 cars, 1 motorcycle, 1 umbrella, 2 handbags, 63.1ms\n","Speed: 3.5ms preprocess, 63.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 2 cars, 1 motorcycle, 2 handbags, 63.1ms\n","Speed: 3.5ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 2 cars, 2 handbags, 59.2ms\n","Speed: 3.4ms preprocess, 59.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 2 cars, 2 handbags, 59.2ms\n","Speed: 3.5ms preprocess, 59.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 3 cars, 1 backpack, 2 handbags, 100.4ms\n","Speed: 5.5ms preprocess, 100.4ms inference, 9.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 4 cars, 2 handbags, 63.1ms\n","Speed: 3.7ms preprocess, 63.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 4 cars, 1 backpack, 1 handbag, 63.1ms\n","Speed: 3.4ms preprocess, 63.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 4 cars, 1 handbag, 55.7ms\n","Speed: 3.6ms preprocess, 55.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 4 cars, 1 backpack, 1 handbag, 55.8ms\n","Speed: 3.4ms preprocess, 55.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 3 cars, 1 backpack, 1 handbag, 55.8ms\n","Speed: 5.4ms preprocess, 55.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 3 cars, 1 backpack, 53.7ms\n","Speed: 3.4ms preprocess, 53.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 4 cars, 1 backpack, 1 handbag, 53.8ms\n","Speed: 3.8ms preprocess, 53.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 4 cars, 1 backpack, 1 handbag, 53.7ms\n","Speed: 3.5ms preprocess, 53.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 10 persons, 5 cars, 1 backpack, 1 handbag, 1 suitcase, 53.7ms\n","Speed: 3.4ms preprocess, 53.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 10 persons, 5 cars, 1 backpack, 1 handbag, 1 suitcase, 63.1ms\n","Speed: 4.4ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 9 persons, 5 cars, 1 backpack, 1 suitcase, 63.1ms\n","Speed: 3.4ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 9 persons, 5 cars, 1 suitcase, 63.1ms\n","Speed: 3.9ms preprocess, 63.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 5 cars, 1 suitcase, 63.1ms\n","Speed: 3.6ms preprocess, 63.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 9 persons, 5 cars, 1 suitcase, 63.1ms\n","Speed: 3.7ms preprocess, 63.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 6 cars, 1 backpack, 1 handbag, 1 suitcase, 63.1ms\n","Speed: 3.5ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 10 persons, 5 cars, 1 suitcase, 63.1ms\n","Speed: 4.4ms preprocess, 63.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 5 cars, 1 suitcase, 63.1ms\n","Speed: 4.9ms preprocess, 63.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 5 cars, 1 traffic light, 1 suitcase, 64.7ms\n","Speed: 3.8ms preprocess, 64.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 5 cars, 1 traffic light, 1 suitcase, 63.9ms\n","Speed: 3.9ms preprocess, 63.9ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 4 cars, 1 traffic light, 1 suitcase, 63.1ms\n","Speed: 3.8ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 4 cars, 1 traffic light, 1 suitcase, 63.1ms\n","Speed: 3.9ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 6 cars, 1 suitcase, 63.1ms\n","Speed: 3.6ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 6 cars, 1 suitcase, 63.1ms\n","Speed: 3.6ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 6 cars, 1 suitcase, 63.1ms\n","Speed: 3.5ms preprocess, 63.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 5 cars, 1 suitcase, 63.1ms\n","Speed: 3.5ms preprocess, 63.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 5 cars, 1 suitcase, 63.1ms\n","Speed: 3.7ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 5 cars, 1 suitcase, 63.1ms\n","Speed: 3.4ms preprocess, 63.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 4 cars, 63.1ms\n","Speed: 3.5ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 5 cars, 63.1ms\n","Speed: 3.7ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 4 cars, 1 backpack, 63.1ms\n","Speed: 3.6ms preprocess, 63.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 4 cars, 1 backpack, 63.1ms\n","Speed: 4.2ms preprocess, 63.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 3 cars, 1 traffic light, 1 backpack, 63.1ms\n","Speed: 3.6ms preprocess, 63.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 3 cars, 1 traffic light, 63.1ms\n","Speed: 3.4ms preprocess, 63.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 1 bicycle, 3 cars, 1 suitcase, 63.1ms\n","Speed: 4.2ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 1 bicycle, 2 cars, 63.1ms\n","Speed: 4.5ms preprocess, 63.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 1 bicycle, 2 cars, 1 backpack, 63.1ms\n","Speed: 3.7ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 1 bicycle, 2 cars, 1 backpack, 1 suitcase, 63.1ms\n","Speed: 3.9ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 1 bicycle, 2 cars, 1 suitcase, 63.1ms\n","Speed: 3.5ms preprocess, 63.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 1 bicycle, 2 cars, 1 suitcase, 63.1ms\n","Speed: 4.9ms preprocess, 63.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 1 bicycle, 2 cars, 1 backpack, 63.1ms\n","Speed: 3.5ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 1 bicycle, 2 cars, 2 backpacks, 63.1ms\n","Speed: 3.5ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 1 bicycle, 2 cars, 1 traffic light, 1 backpack, 63.1ms\n","Speed: 5.7ms preprocess, 63.1ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 1 bicycle, 2 cars, 1 motorcycle, 1 backpack, 63.1ms\n","Speed: 3.5ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 1 bicycle, 2 cars, 1 motorcycle, 2 backpacks, 63.8ms\n","Speed: 3.5ms preprocess, 63.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 1 bicycle, 2 cars, 1 motorcycle, 1 traffic light, 2 backpacks, 63.1ms\n","Speed: 3.5ms preprocess, 63.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 1 bicycle, 1 car, 2 motorcycles, 3 backpacks, 63.1ms\n","Speed: 3.5ms preprocess, 63.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 1 bicycle, 1 car, 1 motorcycle, 1 backpack, 67.1ms\n","Speed: 3.4ms preprocess, 67.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 1 bicycle, 1 car, 1 motorcycle, 2 backpacks, 63.1ms\n","Speed: 3.6ms preprocess, 63.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 1 bicycle, 1 car, 1 motorcycle, 2 backpacks, 63.1ms\n","Speed: 3.9ms preprocess, 63.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 1 bicycle, 1 car, 1 motorcycle, 1 backpack, 63.1ms\n","Speed: 3.7ms preprocess, 63.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 1 bicycle, 1 car, 1 motorcycle, 2 backpacks, 63.1ms\n","Speed: 4.2ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 1 bicycle, 1 car, 1 motorcycle, 1 backpack, 63.1ms\n","Speed: 5.3ms preprocess, 63.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 9 persons, 1 bicycle, 1 car, 1 motorcycle, 1 backpack, 63.1ms\n","Speed: 3.8ms preprocess, 63.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 9 persons, 1 bicycle, 1 car, 1 motorcycle, 2 backpacks, 59.2ms\n","Speed: 3.4ms preprocess, 59.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 1 bicycle, 1 car, 1 motorcycle, 1 backpack, 59.2ms\n","Speed: 3.5ms preprocess, 59.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 1 bicycle, 1 car, 1 motorcycle, 1 backpack, 59.2ms\n","Speed: 3.3ms preprocess, 59.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 1 bicycle, 1 car, 1 motorcycle, 2 backpacks, 59.2ms\n","Speed: 5.1ms preprocess, 59.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 1 bicycle, 1 car, 1 motorcycle, 2 backpacks, 63.1ms\n","Speed: 3.4ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 1 bicycle, 1 car, 1 motorcycle, 1 backpack, 63.1ms\n","Speed: 3.5ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 1 bicycle, 1 car, 1 motorcycle, 1 backpack, 63.1ms\n","Speed: 3.9ms preprocess, 63.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 1 bicycle, 1 car, 2 motorcycles, 2 backpacks, 63.1ms\n","Speed: 3.5ms preprocess, 63.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 1 bicycle, 1 car, 2 motorcycles, 1 backpack, 63.1ms\n","Speed: 3.5ms preprocess, 63.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 1 bicycle, 1 car, 1 motorcycle, 1 backpack, 63.1ms\n","Speed: 3.6ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 car, 1 motorcycle, 2 backpacks, 63.1ms\n","Speed: 3.7ms preprocess, 63.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 car, 1 motorcycle, 1 backpack, 1 suitcase, 63.1ms\n","Speed: 3.5ms preprocess, 63.1ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 car, 1 motorcycle, 1 backpack, 63.1ms\n","Speed: 3.5ms preprocess, 63.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bicycle, 1 car, 1 motorcycle, 1 backpack, 1 suitcase, 63.2ms\n","Speed: 4.2ms preprocess, 63.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bicycle, 1 car, 1 motorcycle, 1 backpack, 63.1ms\n","Speed: 3.9ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bicycle, 1 car, 1 motorcycle, 1 backpack, 63.1ms\n","Speed: 3.5ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 1 bicycle, 1 car, 1 motorcycle, 1 backpack, 63.2ms\n","Speed: 7.7ms preprocess, 63.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bicycle, 1 car, 1 motorcycle, 1 backpack, 63.1ms\n","Speed: 3.6ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bicycle, 1 car, 1 motorcycle, 1 backpack, 63.1ms\n","Speed: 3.8ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bicycle, 1 car, 1 motorcycle, 1 backpack, 63.1ms\n","Speed: 3.6ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bicycle, 1 car, 1 motorcycle, 1 backpack, 1 suitcase, 63.1ms\n","Speed: 4.7ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bicycle, 1 car, 1 motorcycle, 1 backpack, 63.1ms\n","Speed: 4.2ms preprocess, 63.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bicycle, 1 car, 1 motorcycle, 2 backpacks, 68.9ms\n","Speed: 4.4ms preprocess, 68.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 1 bicycle, 1 car, 1 motorcycle, 3 backpacks, 69.9ms\n","Speed: 3.5ms preprocess, 69.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 1 bicycle, 1 car, 1 motorcycle, 2 backpacks, 63.1ms\n","Speed: 3.7ms preprocess, 63.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 1 bicycle, 1 car, 1 motorcycle, 1 backpack, 63.1ms\n","Speed: 5.1ms preprocess, 63.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 1 bicycle, 1 car, 1 motorcycle, 63.1ms\n","Speed: 5.6ms preprocess, 63.1ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bicycle, 1 car, 1 motorcycle, 63.1ms\n","Speed: 4.3ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bicycle, 1 car, 1 motorcycle, 1 backpack, 63.1ms\n","Speed: 3.8ms preprocess, 63.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bicycle, 1 motorcycle, 2 backpacks, 63.1ms\n","Speed: 3.4ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bicycle, 1 car, 1 motorcycle, 1 fire hydrant, 2 backpacks, 1 umbrella, 61.7ms\n","Speed: 3.8ms preprocess, 61.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bicycle, 1 car, 1 motorcycle, 2 backpacks, 1 umbrella, 61.8ms\n","Speed: 6.8ms preprocess, 61.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bicycle, 2 cars, 1 motorcycle, 2 backpacks, 1 umbrella, 61.8ms\n","Speed: 3.7ms preprocess, 61.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 cars, 1 motorcycle, 1 backpack, 1 umbrella, 61.7ms\n","Speed: 4.0ms preprocess, 61.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 1 car, 1 motorcycle, 1 backpack, 1 umbrella, 1 handbag, 61.8ms\n","Speed: 3.6ms preprocess, 61.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 1 car, 1 motorcycle, 1 traffic light, 2 backpacks, 1 umbrella, 1 handbag, 63.1ms\n","Speed: 3.6ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 1 car, 1 motorcycle, 1 backpack, 2 handbags, 63.1ms\n","Speed: 3.9ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 1 bicycle, 1 car, 1 motorcycle, 1 backpack, 1 handbag, 63.1ms\n","Speed: 4.7ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 1 car, 1 motorcycle, 2 backpacks, 1 handbag, 63.1ms\n","Speed: 4.1ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 1 bicycle, 1 car, 1 motorcycle, 2 backpacks, 1 umbrella, 1 handbag, 63.1ms\n","Speed: 4.1ms preprocess, 63.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 1 bicycle, 1 car, 1 motorcycle, 2 backpacks, 1 umbrella, 1 handbag, 63.1ms\n","Speed: 3.6ms preprocess, 63.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 1 bicycle, 1 motorcycle, 1 backpack, 1 umbrella, 1 handbag, 60.5ms\n","Speed: 3.6ms preprocess, 60.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 1 bicycle, 1 motorcycle, 2 backpacks, 1 umbrella, 1 handbag, 60.4ms\n","Speed: 4.3ms preprocess, 60.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 1 bicycle, 1 motorcycle, 1 backpack, 1 umbrella, 1 handbag, 60.4ms\n","Speed: 3.6ms preprocess, 60.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 1 bicycle, 1 car, 1 motorcycle, 1 backpack, 1 umbrella, 1 handbag, 60.5ms\n","Speed: 4.6ms preprocess, 60.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 1 bicycle, 1 car, 1 motorcycle, 1 backpack, 1 umbrella, 1 handbag, 63.1ms\n","Speed: 3.5ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 1 bicycle, 1 car, 1 motorcycle, 1 backpack, 1 umbrella, 1 handbag, 63.1ms\n","Speed: 3.9ms preprocess, 63.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 1 bicycle, 2 cars, 1 motorcycle, 2 backpacks, 1 handbag, 61.8ms\n","Speed: 3.9ms preprocess, 61.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 1 bicycle, 3 cars, 1 motorcycle, 1 backpack, 1 handbag, 61.8ms\n","Speed: 3.6ms preprocess, 61.8ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 1 bicycle, 3 cars, 1 motorcycle, 1 backpack, 1 handbag, 61.8ms\n","Speed: 3.6ms preprocess, 61.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 1 bicycle, 3 cars, 1 motorcycle, 2 backpacks, 1 handbag, 61.7ms\n","Speed: 7.0ms preprocess, 61.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 1 bicycle, 2 cars, 1 motorcycle, 1 backpack, 1 handbag, 69.7ms\n","Speed: 5.0ms preprocess, 69.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 1 bicycle, 2 cars, 1 motorcycle, 1 backpack, 1 handbag, 63.1ms\n","Speed: 3.6ms preprocess, 63.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 1 bicycle, 3 cars, 1 motorcycle, 1 backpack, 1 handbag, 63.1ms\n","Speed: 3.6ms preprocess, 63.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 1 bicycle, 4 cars, 1 motorcycle, 1 backpack, 1 handbag, 63.1ms\n","Speed: 3.6ms preprocess, 63.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 1 bicycle, 3 cars, 1 motorcycle, 1 backpack, 1 handbag, 1 tie, 63.1ms\n","Speed: 3.6ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 1 bicycle, 4 cars, 1 motorcycle, 1 backpack, 60.5ms\n","Speed: 4.2ms preprocess, 60.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 1 bicycle, 4 cars, 1 motorcycle, 2 backpacks, 59.2ms\n","Speed: 3.5ms preprocess, 59.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 1 bicycle, 4 cars, 1 motorcycle, 2 backpacks, 59.2ms\n","Speed: 3.5ms preprocess, 59.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 1 bicycle, 4 cars, 1 motorcycle, 2 backpacks, 59.2ms\n","Speed: 3.8ms preprocess, 59.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 1 bicycle, 4 cars, 1 motorcycle, 1 backpack, 59.2ms\n","Speed: 3.7ms preprocess, 59.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 1 bicycle, 5 cars, 1 motorcycle, 1 backpack, 62.0ms\n","Speed: 3.6ms preprocess, 62.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 1 bicycle, 4 cars, 1 motorcycle, 1 backpack, 63.6ms\n","Speed: 9.0ms preprocess, 63.6ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 1 bicycle, 4 cars, 1 motorcycle, 1 backpack, 63.1ms\n","Speed: 3.9ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 1 bicycle, 5 cars, 1 motorcycle, 1 backpack, 63.1ms\n","Speed: 3.7ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 1 bicycle, 4 cars, 1 motorcycle, 1 backpack, 63.1ms\n","Speed: 3.4ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 9 persons, 1 bicycle, 4 cars, 1 motorcycle, 1 backpack, 60.5ms\n","Speed: 3.8ms preprocess, 60.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 9 persons, 4 cars, 1 motorcycle, 1 backpack, 58.0ms\n","Speed: 3.6ms preprocess, 58.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 9 persons, 4 cars, 1 motorcycle, 1 backpack, 54.7ms\n","Speed: 3.7ms preprocess, 54.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 9 persons, 1 bicycle, 4 cars, 1 motorcycle, 2 backpacks, 54.7ms\n","Speed: 4.9ms preprocess, 54.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 10 persons, 4 cars, 1 motorcycle, 1 backpack, 54.8ms\n","Speed: 3.5ms preprocess, 54.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 10 persons, 3 cars, 1 motorcycle, 1 backpack, 54.8ms\n","Speed: 6.0ms preprocess, 54.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 10 persons, 4 cars, 1 motorcycle, 2 backpacks, 63.1ms\n","Speed: 3.4ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 9 persons, 4 cars, 2 motorcycles, 1 backpack, 63.1ms\n","Speed: 4.2ms preprocess, 63.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 10 persons, 4 cars, 1 motorcycle, 1 backpack, 56.9ms\n","Speed: 5.8ms preprocess, 56.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 10 persons, 4 cars, 1 motorcycle, 56.9ms\n","Speed: 3.5ms preprocess, 56.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 10 persons, 1 bicycle, 4 cars, 1 motorcycle, 2 backpacks, 56.9ms\n","Speed: 3.7ms preprocess, 56.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 1 bicycle, 3 cars, 1 motorcycle, 1 backpack, 56.9ms\n","Speed: 3.6ms preprocess, 56.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 1 bicycle, 4 cars, 1 motorcycle, 63.1ms\n","Speed: 3.6ms preprocess, 63.1ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 1 bicycle, 3 cars, 1 motorcycle, 1 backpack, 63.1ms\n","Speed: 3.4ms preprocess, 63.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 1 bicycle, 4 cars, 1 motorcycle, 2 backpacks, 63.1ms\n","Speed: 4.4ms preprocess, 63.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 1 bicycle, 4 cars, 1 motorcycle, 2 backpacks, 68.0ms\n","Speed: 7.2ms preprocess, 68.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 1 bicycle, 4 cars, 1 motorcycle, 1 backpack, 63.1ms\n","Speed: 4.3ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 1 bicycle, 5 cars, 1 motorcycle, 1 backpack, 55.8ms\n","Speed: 3.6ms preprocess, 55.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 1 bicycle, 5 cars, 1 motorcycle, 1 traffic light, 2 backpacks, 55.8ms\n","Speed: 3.4ms preprocess, 55.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 5 cars, 1 motorcycle, 1 traffic light, 1 backpack, 55.8ms\n","Speed: 3.5ms preprocess, 55.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 3 cars, 1 motorcycle, 1 traffic light, 1 backpack, 55.8ms\n","Speed: 3.6ms preprocess, 55.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 15 persons, 1 bicycle, 2 cars, 1 motorcycle, 1 traffic light, 1 backpack, 63.1ms\n","Speed: 3.6ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 15 persons, 3 cars, 1 motorcycle, 1 traffic light, 63.1ms\n","Speed: 7.7ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 14 persons, 3 cars, 1 motorcycle, 1 backpack, 61.7ms\n","Speed: 3.5ms preprocess, 61.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 1 bicycle, 2 cars, 1 motorcycle, 1 backpack, 52.8ms\n","Speed: 3.8ms preprocess, 52.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 15 persons, 1 bicycle, 2 cars, 1 motorcycle, 1 backpack, 53.1ms\n","Speed: 4.7ms preprocess, 53.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 14 persons, 1 bicycle, 2 cars, 1 motorcycle, 1 backpack, 52.9ms\n","Speed: 3.7ms preprocess, 52.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 1 bicycle, 2 cars, 1 motorcycle, 1 traffic light, 51.0ms\n","Speed: 3.8ms preprocess, 51.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 1 bicycle, 2 cars, 1 motorcycle, 1 traffic light, 1 backpack, 51.0ms\n","Speed: 3.6ms preprocess, 51.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 2 bicycles, 2 cars, 1 motorcycle, 1 handbag, 51.1ms\n","Speed: 3.6ms preprocess, 51.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 1 bicycle, 2 cars, 1 motorcycle, 1 handbag, 51.0ms\n","Speed: 3.4ms preprocess, 51.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 1 bicycle, 2 cars, 1 motorcycle, 1 traffic light, 1 handbag, 49.4ms\n","Speed: 3.5ms preprocess, 49.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 14 persons, 1 bicycle, 2 cars, 1 motorcycle, 1 traffic light, 1 backpack, 1 handbag, 49.5ms\n","Speed: 3.8ms preprocess, 49.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 1 bicycle, 2 cars, 1 motorcycle, 1 traffic light, 1 backpack, 1 handbag, 49.5ms\n","Speed: 4.1ms preprocess, 49.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 1 bicycle, 2 cars, 1 motorcycle, 1 traffic light, 1 backpack, 2 handbags, 49.4ms\n","Speed: 6.5ms preprocess, 49.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 14 persons, 1 bicycle, 2 cars, 1 motorcycle, 1 traffic light, 1 handbag, 58.1ms\n","Speed: 4.1ms preprocess, 58.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 1 bicycle, 2 cars, 1 motorcycle, 1 traffic light, 1 backpack, 1 handbag, 62.8ms\n","Speed: 3.6ms preprocess, 62.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 14 persons, 1 bicycle, 2 cars, 1 motorcycle, 54.8ms\n","Speed: 3.7ms preprocess, 54.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 15 persons, 1 bicycle, 2 cars, 1 motorcycle, 1 backpack, 1 handbag, 54.0ms\n","Speed: 8.1ms preprocess, 54.0ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 14 persons, 1 bicycle, 2 cars, 1 motorcycle, 2 backpacks, 1 handbag, 51.9ms\n","Speed: 3.6ms preprocess, 51.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 14 persons, 1 bicycle, 1 car, 1 motorcycle, 1 backpack, 56.2ms\n","Speed: 5.7ms preprocess, 56.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 1 bicycle, 2 cars, 1 motorcycle, 2 backpacks, 1 handbag, 63.1ms\n","Speed: 3.5ms preprocess, 63.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 1 bicycle, 2 cars, 1 motorcycle, 2 backpacks, 1 handbag, 59.1ms\n","Speed: 3.4ms preprocess, 59.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 1 bicycle, 2 cars, 1 motorcycle, 2 backpacks, 1 handbag, 59.2ms\n","Speed: 3.4ms preprocess, 59.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 1 bicycle, 2 cars, 1 motorcycle, 1 backpack, 1 handbag, 59.2ms\n","Speed: 3.9ms preprocess, 59.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 10 persons, 2 cars, 1 motorcycle, 1 handbag, 51.9ms\n","Speed: 3.4ms preprocess, 51.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 2 cars, 1 motorcycle, 1 backpack, 1 handbag, 51.9ms\n","Speed: 3.5ms preprocess, 51.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 2 cars, 1 motorcycle, 1 backpack, 1 handbag, 51.9ms\n","Speed: 3.5ms preprocess, 51.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 10 persons, 2 cars, 1 motorcycle, 1 backpack, 1 handbag, 51.9ms\n","Speed: 3.5ms preprocess, 51.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 2 cars, 1 motorcycle, 1 backpack, 1 handbag, 63.1ms\n","Speed: 3.5ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 2 cars, 1 motorcycle, 1 traffic light, 1 backpack, 1 handbag, 63.1ms\n","Speed: 4.1ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 2 cars, 1 motorcycle, 1 traffic light, 1 handbag, 61.7ms\n","Speed: 3.8ms preprocess, 61.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 1 bicycle, 2 cars, 1 motorcycle, 1 traffic light, 1 backpack, 1 handbag, 61.7ms\n","Speed: 4.1ms preprocess, 61.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 2 cars, 1 motorcycle, 1 backpack, 1 handbag, 61.7ms\n","Speed: 3.5ms preprocess, 61.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 2 cars, 1 motorcycle, 1 backpack, 1 handbag, 61.7ms\n","Speed: 3.6ms preprocess, 61.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 1 bicycle, 2 cars, 1 motorcycle, 1 backpack, 1 handbag, 63.1ms\n","Speed: 4.6ms preprocess, 63.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 1 bicycle, 1 car, 1 motorcycle, 1 backpack, 1 handbag, 63.1ms\n","Speed: 6.5ms preprocess, 63.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 2 bicycles, 2 cars, 1 motorcycle, 1 backpack, 1 handbag, 51.9ms\n","Speed: 3.6ms preprocess, 51.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 1 bicycle, 2 cars, 1 motorcycle, 1 backpack, 1 handbag, 48.7ms\n","Speed: 6.1ms preprocess, 48.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 1 bicycle, 2 cars, 1 motorcycle, 1 backpack, 50.6ms\n","Speed: 3.6ms preprocess, 50.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 14 persons, 1 bicycle, 2 cars, 1 motorcycle, 1 backpack, 48.6ms\n","Speed: 3.5ms preprocess, 48.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 1 bicycle, 2 cars, 1 motorcycle, 48.6ms\n","Speed: 3.6ms preprocess, 48.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 1 bicycle, 2 cars, 1 motorcycle, 60.4ms\n","Speed: 6.4ms preprocess, 60.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 2 bicycles, 2 cars, 1 motorcycle, 1 backpack, 61.8ms\n","Speed: 4.5ms preprocess, 61.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 1 bicycle, 2 cars, 1 motorcycle, 1 backpack, 63.1ms\n","Speed: 4.2ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 1 bicycle, 2 cars, 1 motorcycle, 1 backpack, 63.1ms\n","Speed: 4.6ms preprocess, 63.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 1 bicycle, 2 cars, 1 motorcycle, 63.1ms\n","Speed: 3.7ms preprocess, 63.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 1 bicycle, 2 cars, 2 motorcycles, 66.6ms\n","Speed: 6.5ms preprocess, 66.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 1 bicycle, 2 cars, 1 motorcycle, 63.1ms\n","Speed: 3.9ms preprocess, 63.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 10 persons, 1 bicycle, 2 cars, 1 motorcycle, 63.1ms\n","Speed: 3.5ms preprocess, 63.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 1 bicycle, 2 cars, 1 motorcycle, 1 chair, 63.6ms\n","Speed: 6.6ms preprocess, 63.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 1 bicycle, 2 cars, 1 motorcycle, 56.9ms\n","Speed: 3.6ms preprocess, 56.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 1 bicycle, 2 cars, 1 motorcycle, 52.5ms\n","Speed: 5.6ms preprocess, 52.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 3 cars, 1 motorcycle, 48.0ms\n","Speed: 3.5ms preprocess, 48.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 3 cars, 1 motorcycle, 47.9ms\n","Speed: 3.5ms preprocess, 47.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 10 persons, 3 cars, 1 motorcycle, 52.2ms\n","Speed: 3.5ms preprocess, 52.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 2 cars, 1 motorcycle, 47.9ms\n","Speed: 3.5ms preprocess, 47.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 10 persons, 3 cars, 1 motorcycle, 48.0ms\n","Speed: 4.3ms preprocess, 48.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 3 cars, 1 motorcycle, 47.9ms\n","Speed: 5.7ms preprocess, 47.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 3 cars, 1 motorcycle, 50.3ms\n","Speed: 4.1ms preprocess, 50.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 4 cars, 1 motorcycle, 52.8ms\n","Speed: 3.8ms preprocess, 52.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 4 cars, 1 motorcycle, 55.8ms\n","Speed: 3.5ms preprocess, 55.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 4 cars, 1 motorcycle, 52.8ms\n","Speed: 3.9ms preprocess, 52.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 3 cars, 1 motorcycle, 51.1ms\n","Speed: 3.9ms preprocess, 51.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 2 cars, 1 motorcycle, 48.7ms\n","Speed: 3.6ms preprocess, 48.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 3 cars, 1 motorcycle, 47.2ms\n","Speed: 3.6ms preprocess, 47.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 2 cars, 1 motorcycle, 47.3ms\n","Speed: 3.5ms preprocess, 47.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 1 bicycle, 3 cars, 1 motorcycle, 47.4ms\n","Speed: 7.0ms preprocess, 47.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 2 bicycles, 3 cars, 1 motorcycle, 47.2ms\n","Speed: 3.5ms preprocess, 47.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 2 bicycles, 3 cars, 1 motorcycle, 47.2ms\n","Speed: 4.1ms preprocess, 47.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 1 bicycle, 3 cars, 1 motorcycle, 47.3ms\n","Speed: 3.3ms preprocess, 47.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 1 bicycle, 3 cars, 1 motorcycle, 47.2ms\n","Speed: 3.6ms preprocess, 47.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 1 bicycle, 3 cars, 1 motorcycle, 47.2ms\n","Speed: 3.9ms preprocess, 47.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 1 bicycle, 3 cars, 1 motorcycle, 47.3ms\n","Speed: 4.3ms preprocess, 47.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 1 bicycle, 2 cars, 1 motorcycle, 47.3ms\n","Speed: 3.5ms preprocess, 47.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 1 bicycle, 2 cars, 1 motorcycle, 47.2ms\n","Speed: 3.5ms preprocess, 47.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 14 persons, 2 bicycles, 2 cars, 1 motorcycle, 50.2ms\n","Speed: 3.8ms preprocess, 50.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 2 bicycles, 2 cars, 1 motorcycle, 62.0ms\n","Speed: 3.5ms preprocess, 62.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 2 bicycles, 2 cars, 1 motorcycle, 63.1ms\n","Speed: 3.8ms preprocess, 63.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 2 bicycles, 2 cars, 1 motorcycle, 63.1ms\n","Speed: 3.3ms preprocess, 63.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 15 persons, 2 bicycles, 2 cars, 1 motorcycle, 57.7ms\n","Speed: 8.1ms preprocess, 57.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 1 bicycle, 2 cars, 1 motorcycle, 47.9ms\n","Speed: 3.5ms preprocess, 47.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 14 persons, 2 cars, 1 motorcycle, 45.3ms\n","Speed: 5.1ms preprocess, 45.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 2 cars, 1 motorcycle, 45.4ms\n","Speed: 3.8ms preprocess, 45.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 14 persons, 3 cars, 1 motorcycle, 45.3ms\n","Speed: 4.5ms preprocess, 45.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 15 persons, 3 cars, 1 motorcycle, 45.3ms\n","Speed: 4.1ms preprocess, 45.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 14 persons, 3 cars, 1 motorcycle, 45.3ms\n","Speed: 4.5ms preprocess, 45.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 14 persons, 3 cars, 1 motorcycle, 54.8ms\n","Speed: 4.1ms preprocess, 54.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 1 bicycle, 3 cars, 1 motorcycle, 56.9ms\n","Speed: 3.6ms preprocess, 56.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 1 bicycle, 3 cars, 1 motorcycle, 59.2ms\n","Speed: 3.5ms preprocess, 59.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 1 bicycle, 3 cars, 1 motorcycle, 60.5ms\n","Speed: 3.4ms preprocess, 60.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 3 cars, 1 motorcycle, 61.8ms\n","Speed: 3.4ms preprocess, 61.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 1 bicycle, 4 cars, 1 motorcycle, 63.2ms\n","Speed: 3.6ms preprocess, 63.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 14 persons, 3 cars, 1 motorcycle, 52.9ms\n","Speed: 3.5ms preprocess, 52.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 14 persons, 3 cars, 1 motorcycle, 51.1ms\n","Speed: 3.4ms preprocess, 51.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 15 persons, 3 cars, 1 motorcycle, 51.1ms\n","Speed: 3.6ms preprocess, 51.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 14 persons, 3 cars, 1 motorcycle, 51.1ms\n","Speed: 3.5ms preprocess, 51.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 16 persons, 3 cars, 1 motorcycle, 51.1ms\n","Speed: 3.5ms preprocess, 51.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 14 persons, 3 cars, 1 motorcycle, 56.9ms\n","Speed: 3.5ms preprocess, 56.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 14 persons, 3 cars, 1 motorcycle, 58.0ms\n","Speed: 3.9ms preprocess, 58.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 14 persons, 3 cars, 1 motorcycle, 60.5ms\n","Speed: 3.6ms preprocess, 60.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 15 persons, 3 cars, 1 motorcycle, 61.8ms\n","Speed: 3.5ms preprocess, 61.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 14 persons, 3 cars, 1 motorcycle, 63.1ms\n","Speed: 3.4ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 14 persons, 3 cars, 1 motorcycle, 63.1ms\n","Speed: 7.8ms preprocess, 63.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 14 persons, 3 cars, 1 motorcycle, 63.1ms\n","Speed: 3.8ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 14 persons, 3 cars, 1 motorcycle, 55.1ms\n","Speed: 3.8ms preprocess, 55.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 3 cars, 1 motorcycle, 51.6ms\n","Speed: 3.4ms preprocess, 51.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 14 persons, 3 cars, 1 motorcycle, 50.2ms\n","Speed: 3.5ms preprocess, 50.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 3 cars, 1 motorcycle, 64.4ms\n","Speed: 3.6ms preprocess, 64.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 4 cars, 1 motorcycle, 63.1ms\n","Speed: 3.4ms preprocess, 63.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 4 cars, 1 motorcycle, 63.1ms\n","Speed: 3.5ms preprocess, 63.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 4 cars, 1 motorcycle, 1 fire hydrant, 61.8ms\n","Speed: 6.8ms preprocess, 61.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 4 cars, 1 motorcycle, 1 fire hydrant, 50.3ms\n","Speed: 8.6ms preprocess, 50.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 1 bicycle, 3 cars, 1 motorcycle, 1 fire hydrant, 50.3ms\n","Speed: 3.9ms preprocess, 50.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 2 bicycles, 3 cars, 1 motorcycle, 1 fire hydrant, 50.3ms\n","Speed: 4.0ms preprocess, 50.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 1 bicycle, 3 cars, 1 motorcycle, 1 fire hydrant, 46.6ms\n","Speed: 3.5ms preprocess, 46.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 1 bicycle, 3 cars, 1 motorcycle, 1 fire hydrant, 46.5ms\n","Speed: 3.7ms preprocess, 46.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 1 bicycle, 3 cars, 1 motorcycle, 46.6ms\n","Speed: 3.6ms preprocess, 46.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 1 bicycle, 3 cars, 1 motorcycle, 44.1ms\n","Speed: 3.5ms preprocess, 44.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 1 bicycle, 3 cars, 1 motorcycle, 43.5ms\n","Speed: 3.5ms preprocess, 43.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 1 bicycle, 3 cars, 1 motorcycle, 42.5ms\n","Speed: 3.5ms preprocess, 42.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 1 bicycle, 4 cars, 1 motorcycle, 42.4ms\n","Speed: 3.5ms preprocess, 42.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 2 bicycles, 4 cars, 1 motorcycle, 42.5ms\n","Speed: 5.8ms preprocess, 42.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 2 bicycles, 4 cars, 1 motorcycle, 42.4ms\n","Speed: 3.5ms preprocess, 42.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 1 bicycle, 3 cars, 1 motorcycle, 42.4ms\n","Speed: 7.9ms preprocess, 42.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 3 cars, 1 motorcycle, 42.4ms\n","Speed: 4.0ms preprocess, 42.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 3 cars, 1 motorcycle, 42.4ms\n","Speed: 3.5ms preprocess, 42.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 1 bicycle, 3 cars, 1 motorcycle, 45.9ms\n","Speed: 3.5ms preprocess, 45.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 14 persons, 1 bicycle, 3 cars, 2 motorcycles, 1 handbag, 44.8ms\n","Speed: 3.6ms preprocess, 44.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 14 persons, 1 bicycle, 3 cars, 1 motorcycle, 1 handbag, 44.1ms\n","Speed: 5.3ms preprocess, 44.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 2 bicycles, 3 cars, 1 motorcycle, 42.2ms\n","Speed: 5.3ms preprocess, 42.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 10 persons, 1 bicycle, 3 cars, 1 motorcycle, 1 handbag, 42.1ms\n","Speed: 3.5ms preprocess, 42.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 10 persons, 1 bicycle, 3 cars, 1 motorcycle, 2 handbags, 42.1ms\n","Speed: 3.4ms preprocess, 42.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 1 bicycle, 3 cars, 1 motorcycle, 1 handbag, 42.2ms\n","Speed: 3.5ms preprocess, 42.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 1 bicycle, 3 cars, 1 motorcycle, 1 handbag, 42.1ms\n","Speed: 3.5ms preprocess, 42.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 1 bicycle, 3 cars, 1 motorcycle, 42.1ms\n","Speed: 3.5ms preprocess, 42.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 14 persons, 3 cars, 1 motorcycle, 1 handbag, 45.6ms\n","Speed: 3.6ms preprocess, 45.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 4 cars, 1 motorcycle, 1 handbag, 46.5ms\n","Speed: 3.6ms preprocess, 46.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 10 persons, 4 cars, 1 motorcycle, 1 handbag, 48.5ms\n","Speed: 3.4ms preprocess, 48.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 4 cars, 1 handbag, 44.0ms\n","Speed: 3.1ms preprocess, 44.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n"]}]},{"cell_type":"code","source":["from google.colab import files\n","files.download(\"person_entry_exit_log.csv\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"Z1ytK2CQ1wZa","executionInfo":{"status":"ok","timestamp":1754744818156,"user_tz":-330,"elapsed":5,"user":{"displayName":"Arkadeb MANNA","userId":"10938977057359683247"}},"outputId":"cf2038b2-494e-4594-a1eb-154853accf48"},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_d7a12a84-0701-4f88-86e8-12993e74959f\", \"person_entry_exit_log.csv\", 903)"]},"metadata":{}}]}]}